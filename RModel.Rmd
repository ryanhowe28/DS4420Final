---
title: "R Model ML2"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
library(keras)
library(tensorflow)
library(caret)

img_width  <- 64   # Adjust as needed
img_height <- 64   # Adjust as needed
channels   <- 3    # RGB images, change to 1 if grayscale
batch_size <- 32L
epochs     <- 30L

data_dir <- "data"

datagen <- image_data_generator(
  rescale = 1/255,              # Normalize pixel values to [0,1]
  rotation_range = 40,          # Random rotation angle
  width_shift_range = 0.2,      # Random horizontal shift
  height_shift_range = 0.2,     # Random vertical shift
  shear_range = 0.2,            # Shear transformation
  zoom_range = 0.2,             # Random zooming
  horizontal_flip = TRUE,       # Random horizontal flip
  fill_mode = "nearest",        # Fill mode for new pixels
  validation_split = 0.2        # Reserve 20% of images for validation
)


train_generator <- flow_images_from_directory(
  directory = data_dir,
  generator = datagen,
  target_size = c(img_width, img_height),
  batch_size = batch_size,
  class_mode = "categorical",
  subset = "training"           # Use training split
)

validation_generator <- flow_images_from_directory(
  directory = data_dir,
  generator = datagen,
  target_size = c(img_width, img_height),
  batch_size = batch_size,
  class_mode = "categorical",
  subset = "validation"         # Use validation split
)

```

```{r}
#Model 

input <- layer_input(shape = c(img_width, img_height, channels))

# Build the model using the functional API
output <- input %>%
  layer_flatten() %>%
  layer_dense(units = 256, activation = "relu") %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 128, activation = "relu") %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = num_classes, activation = "softmax")

# Create the model by specifying inputs and outputs
model <- keras_model(inputs = input, outputs = output)

model$compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_adam(),
  metrics = list("accuracy")
)

```

```{r}
#Train

history <- model$fit(
  x = train_generator,
  steps_per_epoch = as.integer(floor(train_generator$samples / batch_size)),
  epochs = epochs,
  validation_data = validation_generator,
  validation_steps = as.integer(floor(validation_generator$samples / batch_size))
)
```
```{r}
# Extract training history from the Python object
history_list <- history$history

# Extract the loss and validation loss
loss <- history_list$loss
val_loss <- history_list$val_loss
accuracy <- history_list$accuracy
val_accuracy <- history_list$val_accuracy

# Create an epoch sequence
epochs_seq <- seq_along(loss)

# Plot training and validation loss using base R graphics
plot(epochs_seq, loss, type = "l", col = "blue",
     ylim = range(c(loss, val_loss)),
     xlab = "Epoch", ylab = "Loss",
     main = "Training vs. Validation Loss")
lines(epochs_seq, val_loss, type = "l", col = "red")
legend("topright", legend = c("Training Loss", "Validation Loss"),
       col = c("blue", "red"), lty = 1)

plot(epochs_seq, accuracy, type = "l", col = "blue",
     ylim = range(c(accuracy, val_accuracy)),
     xlab = "Epoch", ylab = "Accuracy",
     main = "Training vs. Validation Accuracy")
lines(epochs_seq, val_accuracy, type = "l", col = "red")
legend("bottomright", legend = c("Training Accuracy", "Validation Accuracy"),
       col = c("blue", "red"), lty = 1)

# Calculate the number of validation steps using integer division
val_steps <- as.integer(validation_generator$samples %/% batch_size)

# Evaluate the model by calling the evaluate method directly using the $ operator.
eval_results <- model$evaluate(
  x = validation_generator,
  steps = val_steps
)

# Print the evaluation results
cat("Validation Loss:", eval_results[[1]], "\n")
cat("Validation Accuracy:", eval_results[[2]], "\n")

```

```{r}
#confusion matrix 

validation_datagen <- image_data_generator(rescale = 1/255, validation_split = 0.2)

# Reinitialize the validation generator; note the subset and shuffle parameters.
validation_generator_reset <- flow_images_from_directory(
  directory = data_dir,
  generator = validation_datagen,
  target_size = c(img_width, img_height),
  batch_size = batch_size,
  class_mode = "categorical",
  subset = "validation",   # Ensure this matches your original split if you're using it
  shuffle = FALSE          # We want predictable order for predictions
)

# Predict the class probabilities for the validation set
steps_pred <- as.integer(validation_generator_reset$samples %/% batch_size)
predictions_prob <- model$predict(
  x = validation_generator_reset,
  steps = steps_pred
)

# Convert probabilities to predicted class labels (indices start at 1 in R)
predicted_classes <- apply(predictions_prob, 1, which.max)

# Extract the true classes from the generator
true_classes <- validation_generator_reset$classes[1:length(predicted_classes)]

# Create factors with consistent levels for a confusion matrix
predicted_factor <- factor(predicted_classes, levels = 1:num_classes)
true_factor <- factor(true_classes + 1, levels = 1:num_classes)  # Adjusting since generator classes start at 0

# Compute and print the confusion matrix
conf_matrix <- confusionMatrix(predicted_factor, true_factor)
print(conf_matrix)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

